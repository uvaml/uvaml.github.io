---
layout: post
title: "Rethinking Storage System Design for Modern AI Models"
author: Yue Cheng
homepage: "https://tddg.github.io/"
image: "https://api.dsi.virginia.edu/sites/default/files/styles/square_sm/public/headshots/people/2022-09/YueCheng02-crop.png"
org: "UVA Data Science & CS"
time: 2026-02-25
hour: 12:00 - 13:00 ET
loc: "Rice 540"
zoom: ""
editor: Yen-Ling Kuo
---

**Abstract**
Large-scale model hubs (e.g., Hugging Face, Kaggle) and numerous private repositories collectively host millions of pretrained and fine-tuned models, primarily large language models (LLMs). As the de facto infrastructure for AI model development and sharing, these platforms support a vast ecosystem of downstream applications across research and industry. However, their storage footprint has grown explosively---Hugging Face alone hosted over 70 PB of model artifacts in late 2025 and continues to expand exponentially, posing mounting sustainability challenges. 

In this talk, I will present a new perspective on sustainable, large-scale modern AI model storage that rethinks system design from the ground up. I will first show how fine-tuned models within a model family exhibit high latent redundancy, enabling storage systems to move beyond generic compression toward model-aware data reduction. I will then talk about why model-level assumptions fundamentally break down and why storage systems must be redesigned around a tensor-centric abstraction that minimizes redundancy at the tensor level. Finally, I will share a vision for a tensor-centric AI ecosystem built around the tensor-centric storage infrastructure.

**Bio:**
Yue Cheng holds a dual appointment as an Assistant Professor in the School of Data Science and in the Department of Computer Science. Prior to joining UVA in 2022, he was an Assistant Professor of Computer Science at George Mason University. Cheng's research interests include distributed systems, cloud and serverless computing, high-performance computing, and operating systems. His research is driven by the complexities of modern data-intensive computer systems, and the need for more efficient and easy-to-use approaches to manage such complexities. His current research focuses on designing efficient data systems for data science: (1) designing efficient stateful serverless computing systems using a full-stack approach spanning applications, platforms, and OS/hardware; and (2) building better computing and storage systems for distributed machine learning.

